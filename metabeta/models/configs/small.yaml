# re-usable model configurationgeneral:    d: 3 # number of fixed effects (with bias)    q: 1 # number of random effects (with bias)    m: 30 # maximum number of groups    n: 70 # maximum number of samples per group    varied: False # variable d and q    permute: False # permute slope variables (only relevant for varied input)    standardize: True # use standardization in model pipeline    constrain: True # constrain strictly postivie parameterssummarizer:    type: 'set-transformer' # summarizer architecture [set-transformer, dual-transformer]    n_blocks: 2 # number of transformer blocks    d_model: 128 # embedding dimension    d_ff: 128 # feedforward dimension    d_output: 64 # summary dimension    depth: 1 # feedforward layers    n_heads: 8 # number of attention heads    dropout: 0.01 # dropout rate    activation: 'GELU' # activation function [anything implemented in torch.nn]posterior:    type: 'affine' # normalizing flow architecture [affine, spline]    base: 'student' # base distribution of normalizing flow [gaussian, student]    n_blocks: 6 # number of normalizing flow blocks    d_ff: 256 # feedforward dimension    depth: 3 # feedforward layers    dropout: 0.01 # dropout rate    activation: 'ReLU' # activation function [anything implemented in torch.nn]    net_type: 'mlp' # type of transform parameterizer [mlp, residual]