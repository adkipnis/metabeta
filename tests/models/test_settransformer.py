import pytestimport torchfrom metabeta.models.transformers import SetTransformer# -----------------------# Fixtures# -----------------------@pytest.fixturedef batch_3d():    return torch.randn(8, 10, 3)  # (batch, seq_len, d_input)@pytest.fixturedef batch_4d():    return torch.randn(8, 5, 10, 3)  # (batch, m, seq_len, d_input)@pytest.fixturedef mask_4d():    m = torch.ones((8, 5, 10), dtype=torch.bool)    return m# -----------------------# SetTransformer Tests# -----------------------def test_settransformer_forward_3d(batch_3d):    model = SetTransformer(d_input=3, d_model=16, d_ff=64, n_blocks=4, n_isab=2)    out = model(batch_3d)    assert out.shape == (batch_3d.shape[0], 16)    assert torch.isfinite(out).all()def test_settransformer_forward_4d(batch_4d):    model = SetTransformer(d_input=3, d_model=16, d_ff=64, n_blocks=4, n_isab=2)    out = model(batch_4d)    # Output shape: batch x m x d_model    assert out.shape == (batch_4d.shape[0], batch_4d.shape[1], 16)    assert torch.isfinite(out).all()def test_settransformer_backward_3d(batch_3d):    model = SetTransformer(d_input=3, d_model=16, d_ff=64, n_blocks=4, n_isab=2)    out = model(batch_3d)    loss = out.mean()    loss.backward()    grads = [p.grad for p in model.parameters() if p.requires_grad]    assert any(g is not None for g in grads)    for g in grads:        if g is not None:            assert torch.isfinite(g).all()def test_settransformer_backward_4d(batch_4d):    model = SetTransformer(d_input=3, d_model=16, d_ff=64, n_blocks=4, n_isab=2)    out = model(batch_4d)    loss = out.mean()    loss.backward()    grads = [p.grad for p in model.parameters() if p.requires_grad]    assert any(g is not None for g in grads)    for g in grads:        if g is not None:            assert torch.isfinite(g).all()def test_settransformer_mask_behavior(batch_4d, mask_4d):    model = SetTransformer(d_input=3, d_model=16, d_ff=64, n_blocks=4, n_isab=2)    # forward with full mask    out_full = model(batch_4d, mask=mask_4d)    mask_mod = mask_4d.clone()    mask_mod[..., -1] = False    batch_mod = batch_4d.clone()    batch_mod[~mask_mod] = 99  # entries should be ignored    out_masked = model(batch_mod, mask=mask_mod)    # Outputs corresponding to True mask should match    assert torch.allclose(out_full, out_masked, atol=1e-5)def test_settransformer_with_proj_out(batch_3d):    model = SetTransformer(d_input=3, d_model=16, d_ff=64, n_blocks=4, n_isab=2, d_output=8)    out = model(batch_3d)    assert out.shape == (batch_3d.shape[0], 8)    assert torch.isfinite(out).all()